{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "641a69c8",
   "metadata": {},
   "source": [
    "# CSP Features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7601fcca",
   "metadata": {},
   "source": [
    "### Steps of FB-CSP Classification \n",
    "(Fast EEG-Based Decoding Of The Directional Focus Of Auditory Attention Using Common Spatial Patterns)\n",
    "- Step 1: Filter Bank\n",
    "    - 14(B is number of filter bank) 8th-order butterworth filters. The first filter corresponds to frequency band 1-4 hz. The second with 2-6. And continuous with overlap 2 hz until 26(2+12*2)-30 hz. \n",
    "- Step 2: CSP\n",
    "    - How to choice the filter? RMOE (in this paper) vs. Covariance (mne, or common methods)\n",
    "    - Choice k components from CSP filter\n",
    "- Step 3: Log Energy\n",
    "    - Summation the CSP pattern and calculate the log energy of every band and k filters\n",
    "    - Output feature: Filter Bank B x k components\n",
    "    - This step will be done by mne.decoding.csp.\n",
    "- Step 4: Classification  \n",
    "    - lda, svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09db721",
   "metadata": {},
   "source": [
    "### Data pipline or workflow\n",
    "Theres two task of the works. Binary classification vs. Multi-class\n",
    "First in single classes\n",
    "- Data Prepration: \n",
    "    - Input Data: 2(class) x 25(subjects) x 32/2(trials, unknown?) x channel num x trials timepoint\n",
    "    - Output Data: channel num x 2(class) x segments numbers[subjects x trials x decision windows numbers] x decision windows size (4sec x 128hz)\n",
    "- Step 1: Filter Bank\n",
    "    - Input Data: (channel num, 2 x segments numbers, 512)\n",
    "    - Ouput Data: 2, segments numbers, 14 bank num, 512\n",
    "- Step 2: CSP\n",
    "    - Input Data: (s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc22ecd",
   "metadata": {},
   "source": [
    "### Step 1 - Data Prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2537bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "\n",
    "# Variables\n",
    "window_sec = 4\n",
    "choice_direction = 90 # 0: 全選\n",
    "bands = {\n",
    "    '1': (12, 16),\n",
    "    '2': (14, 18),\n",
    "    '3': (16, 20),\n",
    "    '4': (18, 22),\n",
    "}\n",
    "# for iter_ in range(4, 15):\n",
    "#     pre_low, pre_high = bands[str(iter_-1)]\n",
    "#     bands[str(iter_)] = (pre_low+2, pre_high+2)\n",
    "\n",
    "\n",
    "# Load Data\n",
    "path = \"./NJUNCA_preprocessed_arte_removed/\"\n",
    "expinfo_path = path + \"expinfomat_csvs/\"\n",
    "mat_files = [f for f in os.listdir(path) if f.endswith('.mat')]\n",
    "\n",
    "subjects = 21\n",
    "fs = 128  # sampling frequency\n",
    "window_size = fs * window_sec  # 512\n",
    "channels = 32  # assume full 32 channels\n",
    "count = 0\n",
    "side_dict = {\n",
    "    \"right\": 0,\n",
    "    \"left\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "# Load data\n",
    "all_segments = []\n",
    "all_labels = []\n",
    "all_trials = []\n",
    "pre_trial_idx = 0\n",
    "\n",
    "\n",
    "\n",
    "for subj in range(2, subjects):\n",
    "\n",
    "    print(f\"\\n###  Subject: `{mat_files[subj].split('.')[0]}`\")\n",
    "\n",
    "    print(\"| Trial | Angle (L/R) | Segment Start~End |\")\n",
    "    print(\"|-------|-------------|-------------------|\")\n",
    "\n",
    "    data_path = path + mat_files[subj]\n",
    "    file = h5py.File(data_path, 'r')\n",
    "    csv_path = expinfo_path + mat_files[subj].replace('.mat', '.csv')\n",
    "    df = pd.read_csv(csv_path)['attended_lr']\n",
    "\n",
    "    ref_data = file['data']\n",
    "    ref_eeg = ref_data['eeg']\n",
    "    ref_leftangle = ref_data['event']['leftWav']\n",
    "    ref_rightangle = ref_data['event']['rightWav']\n",
    "    ref_attenside = ref_data['event']['eeg']\n",
    "    trials = len(ref_eeg[:])\n",
    "\n",
    "    for trial in range(trials):\n",
    "        try:\n",
    "            left_angel_reg = file[ref_leftangle['value'][trial][0]]\n",
    "            left_angel = file[left_angel_reg[0][0]][0][0]\n",
    "            right_angel_reg = file[ref_rightangle['value'][trial][0]]\n",
    "            right_angel = file[right_angel_reg[0][0]][0][0]\n",
    "\n",
    "            if (left_angel*-1) != right_angel:\n",
    "                print(f\"| {trial:<5} | {left_angel}/{right_angel:<7} | not the same angle |\")\n",
    "                continue\n",
    "            # if abs(right_angel) != 90:\n",
    "            #     print(f\"{mat_files[subj]}-{trial}: {left_angel}/{right_angel} not 90 degree\")\n",
    "            #     continue\n",
    "            print(f\"| {trial:<5} | {left_angel}/{right_angel:<7} | {pre_trial_idx} ~ {count:<5} |\")\n",
    "            ref = ref_eeg[trial][0]\n",
    "            eeg_data = np.array(file[ref][:]) # shape: 32, time\n",
    "            trial_len = eeg_data.shape[1]\n",
    "            \n",
    "            # Segment into 4-second windows (non-overlapping)\n",
    "            n_windows = trial_len // window_size\n",
    "            \n",
    "            for win in range(n_windows):\n",
    "                segment = eeg_data[:, win * window_size : (win + 1) * window_size]  # shape: (channels, 512)\n",
    "                count += 1\n",
    "                all_segments.append(segment)\n",
    "                all_labels.append(side_dict[df[trial]])\n",
    "                \n",
    "            # write trial info dict\n",
    "            trial_info = {\n",
    "                \"subject_name\": mat_files[subj].split('.')[0],\n",
    "                \"trial\":trial,\n",
    "                \"attention_side\": df[trial],\n",
    "                \"left_angel\": left_angel,\n",
    "                \"right_angel\": right_angel,\n",
    "                \"trial_start\": pre_trial_idx,\n",
    "                \"trial_end\": count,\n",
    "            }\n",
    "            all_trials.append(trial_info)\n",
    "            pre_trial_idx = count \n",
    "        except:\n",
    "            print(f\"| {trial:<5} | {left_angel}/{right_angel:<7} | Load error!! |\")\n",
    "    print(f\"\\n**Total Segments ({mat_files[subj].split('.')[0]}):** {count}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.stack(all_segments)  # shape: (total_segments, channels, 512)\n",
    "y = np.array(all_labels)    # shape: (total_segments,)\n",
    "\n",
    "print(\"---\\n\")\n",
    "print(\"** Dataset Summary**\")\n",
    "print(f\"- **EEG Shape:** {X.shape}\")\n",
    "print(f\"- **Labels Shape:** {y.shape}\")\n",
    "\n",
    "n_epochs, n_channels, n_times = X.shape\n",
    "sfreq = 128  # sampling frequency\n",
    "\n",
    "# 建立 channel info\n",
    "# 你給的通道名稱\n",
    "ch_names = [\n",
    "    'Cz', 'Fz', 'Fp1', 'F7', 'F3', 'FC1', 'C3', 'FC5', 'FT9', 'T7', 'CP5', 'CP1',\n",
    "    'P3', 'P7', 'PO9', 'O1', 'Pz', 'Oz', 'O2', 'PO10', 'P8', 'P4', 'CP2', 'CP6',\n",
    "    'T8', 'FT10', 'FC6', 'C4', 'FC2', 'F4', 'F8', 'Fp2'\n",
    "]\n",
    "ch_types = ['eeg'] * n_channels\n",
    "info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "\n",
    "# 建立 event array\n",
    "# MNE 的 events shape 是 (n_epochs, 3)，通常中間值無用\n",
    "# 最後一欄是 class label，需要轉換成 event id 後對應 dict\n",
    "events = np.column_stack((\n",
    "    np.arange(len(y)),  # arbitrary sample indices\n",
    "    np.zeros(len(y), dtype=int),  # filler\n",
    "    y\n",
    "))\n",
    "\n",
    "# 建立 event_id 字典\n",
    "event_id = {f'class_{i}': i for i in np.unique(y)}\n",
    "\n",
    "# 轉成 EpochsArray\n",
    "epochs = mne.EpochsArray(X, info, events=events, event_id=event_id, verbose=False)\n",
    "montage = mne.channels.make_standard_montage('standard_1020')\n",
    "epochs.set_montage(montage)\n",
    "\n",
    "print(\"- **Epochs Info:**\")\n",
    "print(f\"  - `Total Epochs:` {len(epochs)}\")\n",
    "for label, count_label in event_id.items():\n",
    "    print(f\"  - `{label}`: {(y == count_label).sum()}\")\n",
    "print(f\"  - `Duration:` 0 – {X.shape[2] / sfreq:.3f} s\")\n",
    "print(f\"  - `Memory:` ~{X.nbytes / (1024 ** 2):.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b4e1da",
   "metadata": {},
   "source": [
    "### Subject Group Training set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee51129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_ = [(2, 7), (8, 15), (16, 19), (21, 23), (25, 27)]  # 5 groups\n",
    "group_index_list = [[] for _ in range(len(group_))]      # list of 5 empty lists\n",
    "\n",
    "cum_trials = 0 \n",
    "for idx, info_dict in enumerate(all_trials):\n",
    "    # print(f\"{idx}-{info_dict['subject_name']}/{info_dict['trial']} - cumulated trials rate:{info_dict['trial_end']/count}\")\n",
    "    if choice_direction != 0:\n",
    "        if abs(info_dict['left_angel']) != choice_direction:\n",
    "            continue\n",
    "\n",
    "    subject_int = int(info_dict['subject_name'].replace('S', ''))\n",
    "\n",
    "    # 分配到對應的 group\n",
    "    for group_id, (low, high) in enumerate(group_):\n",
    "        if low <= subject_int <= high:\n",
    "            trial_range = range(info_dict['trial_start'], info_dict['trial_end'])\n",
    "            # print(info_dict['subject_name'], info_dict['trial'], info_dict['trial_start'], info_dict['trial_end'])\n",
    "            cum_trials += len(trial_range)\n",
    "            group_index_list[group_id].extend(trial_range)\n",
    "            break\n",
    "print(f\"## Total Segments: {cum_trials}\")\n",
    "print(\"| Group | Subject Range | Segment Count | Segment Ratio (%) | Cumulated Ratio (%) |\")\n",
    "print(\"|-------|---------------|---------------|-------------------|---------------------|\")\n",
    "cum_ratio = 0\n",
    "for idx, g_l in enumerate(group_index_list):\n",
    "    trial_ratio = len(g_l) / cum_trials\n",
    "    cum_ratio += trial_ratio\n",
    "    low, high = group_[idx]\n",
    "    print(f\"| {idx+1}     | S{low:02d} ~ S{high:02d}     | {len(g_l):<13} | {trial_ratio*100:>8.1f}%         | {cum_ratio*100:>5.1f}%              |\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103548e5",
   "metadata": {},
   "source": [
    "### Step 2 - CSP and Log Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5af4b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn import svm, tree\n",
    "from mne.decoding import CSP\n",
    "\n",
    "def do_csp(bands, train_epochs, test_epochs, train_y, test_y):\n",
    "    csp_list = []\n",
    "    train_features = []\n",
    "    test_features = []\n",
    "\n",
    "    for name, (l_freq, h_freq) in bands.items():\n",
    "        train_epochs_band = train_epochs.copy().filter(l_freq, h_freq, fir_design='firwin').get_data(copy=False)\n",
    "        test_epochs_band = test_epochs.copy().filter(l_freq, h_freq, fir_design='firwin').get_data(copy=False)\n",
    "\n",
    "        csp = CSP(n_components=4, reg='ledoit_wolf')\n",
    "        train_csp = csp.fit_transform(train_epochs_band, train_y)\n",
    "        test_csp = csp.transform(test_epochs_band)\n",
    "\n",
    "        csp_list.append(csp)\n",
    "        train_features.append(train_csp)\n",
    "        test_features.append(test_csp)\n",
    "\n",
    "    train_CSP_features = np.concatenate(train_features, axis=1)\n",
    "    test_CSP_features = np.concatenate(test_features, axis=1)\n",
    "\n",
    "    cv = ShuffleSplit(10, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define classifiers\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    svc = svm.SVC()\n",
    "    dt = tree.DecisionTreeClassifier()\n",
    "\n",
    "    # Cross-validation scores (optional, not used in return)\n",
    "    _ = cross_val_score(svc, train_CSP_features, train_y, cv=cv)\n",
    "    _ = cross_val_score(lda, train_CSP_features, train_y, cv=cv)\n",
    "    _ = cross_val_score(dt, train_CSP_features, train_y, cv=cv)\n",
    "\n",
    "    # Fit and predict\n",
    "    svc.fit(train_CSP_features, train_y)\n",
    "    lda.fit(train_CSP_features, train_y)\n",
    "    dt.fit(train_CSP_features, train_y)\n",
    "\n",
    "    svm_pred_y = svc.predict(test_CSP_features)\n",
    "    lda_pred_y = lda.predict(test_CSP_features)\n",
    "    dt_pred_y = dt.predict(test_CSP_features)\n",
    "\n",
    "    return svm_pred_y, lda_pred_y, dt_pred_y, csp_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71ae2b7",
   "metadata": {},
   "source": [
    "### Step 3 - Do Leave one group out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a558a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def leaveonegroupout(group_index, epochs, y, bands):\n",
    "    pred_svm = []\n",
    "    pred_lda = []\n",
    "    pred_dt = []\n",
    "    true_labels = []\n",
    "    csp_list = []\n",
    "\n",
    "    acc_svm_list, acc_lda_list, acc_dt_list = [], [], []\n",
    "    f1_svm_list, f1_lda_list, f1_dt_list = [], [], []\n",
    "\n",
    "    for valid_group in range(len(group_index)):\n",
    "        print(f\"--------------------------------------------------------------------------------------------------------------\\n\\n\")\n",
    "        print(f\"\\n\\tGroup:{valid_group}\\n\")\n",
    "        print(f\"--------------------------------------------------------------------------------------------------------------\\n\\n\")\n",
    "\n",
    "        test_idx = group_index[valid_group]\n",
    "        train_idx = [i for g, indices in enumerate(group_index) if g != valid_group for i in indices]\n",
    "\n",
    "        train_epochs = epochs[train_idx].copy()\n",
    "        test_epochs = epochs[test_idx].copy()\n",
    "        train_y = y[train_idx]\n",
    "        test_y = y[test_idx]\n",
    "\n",
    "        temp_svm_pred_y, temp_lda_pred_y, temp_dt_pred_y, temp_csp_list = do_csp(bands, train_epochs, test_epochs, train_y, test_y)\n",
    "\n",
    "        pred_svm.extend(temp_svm_pred_y)\n",
    "        pred_lda.extend(temp_lda_pred_y)\n",
    "        pred_dt.extend(temp_dt_pred_y)\n",
    "        csp_list.extend(temp_csp_list)\n",
    "        true_labels.extend(test_y)\n",
    "\n",
    "        # Accuracy\n",
    "        acc_svm = accuracy_score(test_y, temp_svm_pred_y)\n",
    "        acc_lda = accuracy_score(test_y, temp_lda_pred_y)\n",
    "        acc_dt = accuracy_score(test_y, temp_dt_pred_y)\n",
    "\n",
    "        # F1\n",
    "        f1_svm = f1_score(test_y, temp_svm_pred_y, average='macro')\n",
    "        f1_lda = f1_score(test_y, temp_lda_pred_y, average='macro')\n",
    "        f1_dt = f1_score(test_y, temp_dt_pred_y, average='macro')\n",
    "\n",
    "        # Append to lists\n",
    "        acc_svm_list.append(acc_svm)\n",
    "        acc_lda_list.append(acc_lda)\n",
    "        acc_dt_list.append(acc_dt)\n",
    "\n",
    "        f1_svm_list.append(f1_svm)\n",
    "        f1_lda_list.append(f1_lda)\n",
    "        f1_dt_list.append(f1_dt)\n",
    "\n",
    "    # Global metrics\n",
    "    pred_svm = np.array(pred_svm)\n",
    "    pred_lda = np.array(pred_lda)\n",
    "    pred_dt = np.array(pred_dt)\n",
    "    true_labels = np.array(true_labels)\n",
    "\n",
    "    acc_svm_total = accuracy_score(true_labels, pred_svm)\n",
    "    acc_lda_total = accuracy_score(true_labels, pred_lda)\n",
    "    acc_dt_total = accuracy_score(true_labels, pred_dt)\n",
    "\n",
    "    f1_svm_total = f1_score(true_labels, pred_svm, average='macro')\n",
    "    f1_lda_total = f1_score(true_labels, pred_lda, average='macro')\n",
    "    f1_dt_total = f1_score(true_labels, pred_dt, average='macro')\n",
    "\n",
    "    # Std\n",
    "    std_acc_svm = np.std(acc_svm_list)\n",
    "    std_acc_lda = np.std(acc_lda_list)\n",
    "    std_acc_dt = np.std(acc_dt_list)\n",
    "\n",
    "    std_f1_svm = np.std(f1_svm_list)\n",
    "    std_f1_lda = np.std(f1_lda_list)\n",
    "    std_f1_dt = np.std(f1_dt_list)\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\" Classification Results Summary\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Model':<13}  | {'Accuracy':>9} | {'Std(acc)':>9} | {'F1-score':>9} | {'Std(F1)':>9}\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'SVM':<13}  | {acc_svm_total:>9.3f} | {std_acc_svm:>9.3f} | {f1_svm_total:>9.3f} | {std_f1_svm:>9.3f}\")\n",
    "    print(f\"{'LDA':<13}  | {acc_lda_total:>9.3f} | {std_acc_lda:>9.3f} | {f1_lda_total:>9.3f} | {std_f1_lda:>9.3f}\")\n",
    "    print(f\"{'Decision Tree'}  | {acc_dt_total:>9.3f} | {std_acc_dt:>9.3f} | {f1_dt_total:>9.3f} | {std_f1_dt:>9.3f}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(\"\\n Group-wise Accuracy (per fold):\")\n",
    "    print(f\"SVM           : {[round(a, 3) for a in acc_svm_list]}\")\n",
    "    print(f\"LDA           : {[round(a, 3) for a in acc_lda_list]}\")\n",
    "    print(f\"Decision Tree : {[round(a, 3) for a in acc_dt_list]}\")\n",
    "\n",
    "    \n",
    "    return acc_svm_total, f1_svm, acc_lda_total, f1_lda, acc_dt_total, f1_dt, csp_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331341b9",
   "metadata": {},
   "source": [
    "### RUN FB CSP Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41a04fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, _, _, _, csp_list = leaveonegroupout(group_index_list, epochs, y, bands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6649b78",
   "metadata": {},
   "source": [
    "### Plot CSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6959b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, csp in enumerate(csp_list):\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\t\\t\\t\\t Group {int(idx/len(bands))+1}\")\n",
    "    print(f\"\\t\\t\\t Filter Band: {bands[str((idx)%len(bands)+1)]}\")\n",
    "    print(\"=\"*100)\n",
    "    csp.plot_patterns(epochs.info, ch_type=\"eeg\", units=\"Patterns (AU)\", size=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cad14f",
   "metadata": {},
   "source": [
    "### Mulit-class CSP\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
